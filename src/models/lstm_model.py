"""
PyTorch LSTM ëª¨ë¸

í˜‘ì—… í¬ì¸íŠ¸:
- ML ì—”ì§€ë‹ˆì–´: ì´ ëª¨ë¸ í•™ìŠµ ë° íŠœë‹
- ë°±ì—”ë“œ: í•™ìŠµëœ .pt íŒŒì¼ ë¡œë“œí•´ì„œ APIì—ì„œ ì˜ˆì¸¡
- í”„ë¡ íŠ¸: API í˜¸ì¶œ â†’ ì˜ˆì¸¡ ê²°ê³¼ ì‹œê°í™”
"""
import torch
import torch.nn as nn
from typing import Tuple


class StockLSTM(nn.Module):
    """
    ì£¼ì‹ ê°€ê²© ì˜ˆì¸¡ìš© LSTM ëª¨ë¸
    
    êµ¬ì¡°:
    Input â†’ LSTM Layer 1 â†’ Dropout â†’ LSTM Layer 2 â†’ Dropout â†’ FC â†’ Output
    
    ë”¥ëŸ¬ë‹ ì´ë¡ :
    - 2ê°œ LSTM Layer: ë” ë³µì¡í•œ íŒ¨í„´ í•™ìŠµ
    - Dropout: ê³¼ì í•© ë°©ì§€ (ëœë¤í•˜ê²Œ ë‰´ëŸ° ë„ê¸°)
    - FC (Fully Connected): ìµœì¢… ì˜ˆì¸¡ê°’ ìƒì„±
    """
    
    def __init__(
        self,
        input_size: int,      # Feature ê°œìˆ˜ (32)
        hidden_size: int,     # LSTM hidden unit ê°œìˆ˜ (128)
        num_layers: int,      # LSTM layer ê°œìˆ˜ (2)
        dropout: float,       # Dropout ë¹„ìœ¨ (0.2)
        output_size: int = 1  # ì¶œë ¥ ê°œìˆ˜ (1, ì¢…ê°€ë§Œ ì˜ˆì¸¡)
    ):
        """
        Args:
            input_size: ì…ë ¥ Feature ê°œìˆ˜
            hidden_size: LSTM hidden state í¬ê¸°
            num_layers: LSTM ë ˆì´ì–´ ê°œìˆ˜
            dropout: Dropout ë¹„ìœ¨ (0~1)
            output_size: ì¶œë ¥ í¬ê¸° (ê¸°ë³¸ 1, ì¢…ê°€ ì˜ˆì¸¡)
        """
        super(StockLSTM, self).__init__()
        
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        
        # LSTM Layer
        """
        batch_first=True: 
        - ì…ë ¥ shape: (batch_size, sequence_length, input_size)
        - Falseë©´: (sequence_length, batch_size, input_size)
        - batch_first=Trueê°€ ì§ê´€ì !
        
        dropout:
        - LSTM layer ì‚¬ì´ì—ë§Œ ì ìš©
        - ë§ˆì§€ë§‰ layerì—ëŠ” ì ìš© ì•ˆ ë¨
        
        bidirectional:
        - False: ê³¼ê±°â†’ë¯¸ë˜ ë°©í–¥ë§Œ
        - True: ê³¼ê±°â†’ë¯¸ë˜, ë¯¸ë˜â†’ê³¼ê±° ì–‘ë°©í–¥
        - ì£¼ì‹ì€ ë¯¸ë˜ ë°ì´í„° ëª» ì“°ë‹ˆê¹Œ False!
        """
        self.lstm = nn.LSTM(
            input_size=input_size,
            hidden_size=hidden_size,
            num_layers=num_layers,
            batch_first=True,
            dropout=dropout if num_layers > 1 else 0
        )
        
        # Dropout Layer
        """
        ê³¼ì í•©(Overfitting) ë°©ì§€:
        - í•™ìŠµ ì‹œ: ëœë¤í•˜ê²Œ 20% ë‰´ëŸ° ë¹„í™œì„±í™”
        - í…ŒìŠ¤íŠ¸ ì‹œ: ëª¨ë“  ë‰´ëŸ° ì‚¬ìš©
        - ë” ì¼ë°˜í™”ëœ ëª¨ë¸ í•™ìŠµ
        """
        self.dropout = nn.Dropout(dropout)
        
        # Fully Connected Layer
        """
        LSTM ì¶œë ¥ â†’ ìµœì¢… ì˜ˆì¸¡ê°’
        - ì…ë ¥: hidden_size (128)
        - ì¶œë ¥: 1 (ì˜ˆì¸¡ ì¢…ê°€)
        """
        self.fc = nn.Linear(hidden_size, output_size)
        
    def forward(
        self,
        x: torch.Tensor,
        hidden: Tuple[torch.Tensor, torch.Tensor] = None
    ) -> Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:
        """
        ìˆœì „íŒŒ (Forward Pass)
        
        ë°ì´í„° íë¦„:
        1. ì…ë ¥ x â†’ LSTM
        2. LSTM ì¶œë ¥ â†’ Dropout
        3. Dropout ì¶œë ¥ â†’ FC Layer
        4. FC ì¶œë ¥ = ìµœì¢… ì˜ˆì¸¡
        
        Args:
            x: ì…ë ¥ í…ì„œ (batch_size, sequence_length, input_size)
               ì˜ˆ: (32, 60, 32)
            hidden: ì´ˆê¸° hidden state (ì„ íƒ, ë³´í†µ None)
            
        Returns:
            output: ì˜ˆì¸¡ê°’ (batch_size, 1)
            hidden: ë§ˆì§€ë§‰ hidden state (ë‚˜ì¤‘ì— ì—°ì† ì˜ˆì¸¡ ì‹œ ì‚¬ìš©)
        """
        # LSTM í†µê³¼
        """
        lstm_out shape: (batch_size, sequence_length, hidden_size)
                        (32, 60, 128)
        
        ê° íƒ€ì„ìŠ¤í…ë§ˆë‹¤ ì¶œë ¥ì´ ë‚˜ì˜¤ì§€ë§Œ,
        ìš°ë¦¬ëŠ” ë§ˆì§€ë§‰ íƒ€ì„ìŠ¤í…(60ì¼ì§¸)ë§Œ ì‚¬ìš©!
        """
        if hidden is None:
            lstm_out, hidden = self.lstm(x)
        else:
            lstm_out, hidden = self.lstm(x, hidden)
        
        # ë§ˆì§€ë§‰ íƒ€ì„ìŠ¤í…ì˜ ì¶œë ¥ë§Œ ì¶”ì¶œ
        """
        lstm_out[:, -1, :]:
        - [:, -1, :] = ëª¨ë“  ë°°ì¹˜ì˜ ë§ˆì§€ë§‰ íƒ€ì„ìŠ¤í…
        - shape: (batch_size, hidden_size)
        -         (32, 128)
        """
        last_output = lstm_out[:, -1, :]
        
        # Dropout ì ìš©
        """
        ê³¼ì í•© ë°©ì§€
        í•™ìŠµ ì‹œì—ë§Œ ì‘ë™ (model.train())
        """
        dropped = self.dropout(last_output)
        
        # Fully Connected Layer â†’ ìµœì¢… ì˜ˆì¸¡
        """
        (32, 128) â†’ (32, 1)
        ê° ìƒ˜í”Œë§ˆë‹¤ 1ê°œ ì˜ˆì¸¡ê°’
        """
        output = self.fc(dropped)
        
        return output, hidden
    
    def init_hidden(self, batch_size: int, device: str = 'cpu') -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Hidden state ì´ˆê¸°í™”
        
        LSTMì€ 2ê°œ state í•„ìš”:
        1. h_0: hidden state (ë‹¨ê¸° ê¸°ì–µ)
        2. c_0: cell state (ì¥ê¸° ê¸°ì–µ)
        
        Args:
            batch_size: ë°°ì¹˜ í¬ê¸°
            device: 'cpu' or 'cuda'
            
        Returns:
            (h_0, c_0): ì´ˆê¸°í™”ëœ hidden states
        """
        h_0 = torch.zeros(
            self.num_layers,
            batch_size,
            self.hidden_size
        ).to(device)
        
        c_0 = torch.zeros(
            self.num_layers,
            batch_size,
            self.hidden_size
        ).to(device)
        
        return (h_0, c_0)


# ëª¨ë¸ í…ŒìŠ¤íŠ¸
if __name__ == "__main__":
    print("="*60)
    print("ğŸ§  LSTM ëª¨ë¸ í…ŒìŠ¤íŠ¸")
    print("="*60)
    
    # í•˜ì´í¼íŒŒë¼ë¯¸í„°
    input_size = 32      # Feature ê°œìˆ˜
    hidden_size = 128    # LSTM hidden units
    num_layers = 2       # LSTM layers
    dropout = 0.2        # Dropout ë¹„ìœ¨
    sequence_length = 60 # ì…ë ¥ ì‹œí€€ìŠ¤ ê¸¸ì´
    batch_size = 32      # ë°°ì¹˜ í¬ê¸°
    
    print(f"\nğŸ“Œ ëª¨ë¸ ì„¤ì •:")
    print(f"   Input Size: {input_size}")
    print(f"   Hidden Size: {hidden_size}")
    print(f"   Num Layers: {num_layers}")
    print(f"   Dropout: {dropout}")
    
    # ëª¨ë¸ ìƒì„±
    model = StockLSTM(
        input_size=input_size,
        hidden_size=hidden_size,
        num_layers=num_layers,
        dropout=dropout
    )
    
    print(f"\nğŸ—ï¸  ëª¨ë¸ êµ¬ì¡°:")
    print(model)
    
    # íŒŒë¼ë¯¸í„° ê°œìˆ˜ ê³„ì‚°
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    
    print(f"\nğŸ“Š íŒŒë¼ë¯¸í„°:")
    print(f"   ì „ì²´: {total_params:,}")
    print(f"   í•™ìŠµ ê°€ëŠ¥: {trainable_params:,}")
    
    # ë”ë¯¸ ë°ì´í„°ë¡œ í…ŒìŠ¤íŠ¸
    print(f"\nğŸ§ª ìˆœì „íŒŒ í…ŒìŠ¤íŠ¸:")
    dummy_input = torch.randn(batch_size, sequence_length, input_size)
    print(f"   ì…ë ¥ shape: {dummy_input.shape}")
    
    # ìˆœì „íŒŒ
    output, hidden = model(dummy_input)
    
    print(f"   ì¶œë ¥ shape: {output.shape}")
    print(f"   Hidden state shape: {hidden[0].shape}")
    print(f"   Cell state shape: {hidden[1].shape}")
    
    # ì˜ˆì¸¡ê°’ í™•ì¸
    print(f"\nğŸ¯ ì˜ˆì¸¡ê°’ ìƒ˜í”Œ (ì •ê·œí™”ëœ ê°’):")
    print(f"   {output[:5].squeeze().detach().numpy()}")
    
    print(f"\nâœ… ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    print("="*60)